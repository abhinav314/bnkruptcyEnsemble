{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline\n",
    "from matplotlib import*\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import *\n",
    "from IPython.display import Image, display_svg\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "from os import path\n",
    "import ipywidgets as widgets\n",
    "import pandas_profiling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.vq import kmeans, vq , whiten\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy as sp\n",
    "import ipywidgets as widgets\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import deepcopy\n",
    "from sklearn import tree\n",
    "import collections\n",
    "import h2o\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\abhin\\\\Desktop\\\\M571\\\\bankruptcy_Train.csv')\n",
    "df.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = df.drop(\"class\", axis=1), df[\"class\"]\n",
    "\n",
    "x_resampled, y_resampled = SMOTE().fit_resample(x1, y1)\n",
    "x_resampled = pd.DataFrame(x_resampled)\n",
    "features = ['Attr1', 'Attr2', 'Attr3', 'Attr4', 'Attr5', 'Attr6', 'Attr7', 'Attr8',\n",
    "       'Attr9', 'Attr10', 'Attr11', 'Attr12', 'Attr13', 'Attr14', 'Attr15',\n",
    "       'Attr16', 'Attr17', 'Attr18', 'Attr19', 'Attr20', 'Attr21', 'Attr22',\n",
    "       'Attr23', 'Attr24', 'Attr25', 'Attr26', 'Attr27', 'Attr28', 'Attr29',\n",
    "       'Attr30', 'Attr31', 'Attr32', 'Attr33', 'Attr34', 'Attr35', 'Attr36',\n",
    "       'Attr37', 'Attr38', 'Attr39', 'Attr40', 'Attr41', 'Attr42', 'Attr43',\n",
    "       'Attr44', 'Attr45', 'Attr46', 'Attr47', 'Attr48', 'Attr49', 'Attr50',\n",
    "       'Attr51', 'Attr52', 'Attr53', 'Attr54', 'Attr55', 'Attr56', 'Attr57',\n",
    "       'Attr58', 'Attr59', 'Attr60', 'Attr61', 'Attr62', 'Attr63', 'Attr64']\n",
    "x_resampled.columns = features\n",
    "y_resampled = pd.DataFrame(y_resampled)\n",
    "y_resampled.columns = [\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, y1 = df.drop(\"class\", axis=1), df[\"class\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=12345)\n",
    "eval_set = [(x_train, y_train)]\n",
    "eval_metric = [\"auc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run baseline XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg = XGBClassifier()\n",
    "xg_reg.fit(x_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   58.3s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done 120 out of 120 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.0,\n",
      "              learning_rate=0.2, max_delta_step=0, max_depth=15,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "params = {\"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\": [ 1, 3, 5, 7 ],\n",
    " \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\": [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "folds = 3\n",
    "param_comb = 40\n",
    "data_dmatrix = xgb.DMatrix(data=x_train,label=y_train)\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "random_search = RandomizedSearchCV(xg_reg, param_distributions=params, n_iter=param_comb, scoring='roc_auc',\n",
    "                                   n_jobs=4, cv=skf.split(x_train,y_train), verbose=10, random_state=1001 )\n",
    "random_search.fit(x_train, y_train)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.99994\n",
      "[200]\tvalid's auc: 0.999976\n",
      "Early stopping, best iteration is:\n",
      "[215]\tvalid's auc: 0.999979\n",
      "Best score reached: 0.9999380629446257 with params: {'colsample_bytree': 0.952164731370897, 'min_child_samples': 111, 'min_child_weight': 0.01, 'num_leaves': 38, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.3029313662262354} \n"
     ]
    }
   ],
   "source": [
    "# lgbm tune\n",
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [(x_test,y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 1001)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf.split(x_train,y_train),\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=10, n_jobs=-1)\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "gs.fit(x_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  75 | elapsed: 16.4min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed: 20.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.9449882972981684 with params: {'subsample': 0.95, 'n_estimators': 130, 'min_samples_split': 0.46363636363636374, 'min_samples_leaf': 0.24545454545454548, 'max_features': 'log2', 'max_depth': 8, 'loss': 'deviance', 'learning_rate': 0.15, 'criterion': 'friedman_mse'} \n"
     ]
    }
   ],
   "source": [
    "#gbm tune\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10, 40, 70, 100, 130, 160, 190]\n",
    "    }\n",
    "n_HP_points_to_test = 25\n",
    "skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 1001)\n",
    "gclf = GradientBoostingClassifier(max_depth=-1, random_state=314, n_estimators=5000)\n",
    "gbtune = RandomizedSearchCV(\n",
    "    estimator=gclf, param_distributions=parameters, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf.split(x_train,y_train),\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=10, n_jobs=-1)\n",
    "\n",
    "gbtune.fit(x_train, y_train)\n",
    "print('Best score reached: {} with params: {} '.format(gbtune.best_score_, gbtune.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:    3.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.9553835297776863 with params: {'n_estimators': 300, 'learning_rate': 0.05} \n"
     ]
    }
   ],
   "source": [
    "#ada tune\n",
    "param_dist = {\n",
    " 'n_estimators': [50, 100, 200, 300],\n",
    " 'learning_rate' : [0.01,0.05,0.1,0.3,1]\n",
    " }\n",
    "n_HP_points_to_test = 40\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n",
    "DTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "ada = AdaBoostClassifier(base_estimator = DTC)\n",
    "abtune = RandomizedSearchCV(\n",
    "    estimator=ada, param_distributions=param_dist, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf.split(x_train,y_train),\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=10, n_jobs=-1)\n",
    "\n",
    "abtune.fit(x_train, y_train)\n",
    "print('Best score reached: {} with params: {} '.format(abtune.best_score_, abtune.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n",
    "#               learning_rate=0.11, max_delta_step=0.5, max_depth=3,\n",
    "#               min_child_weight=2, missing=None, n_estimators=300, n_jobs=-1,\n",
    "#               objective='binary:logistic', random_state=0,\n",
    "#               reg_alpha=0.3, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=False, subsample=0.8, verbosity=1)\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.0,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "              min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
    "              nthread=None, objective='binary:logistic', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=None, subsample=1, verbosity=1)\n",
    "xgb.fit(x_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=0.952164731370897, importance_type='split',\n",
       "               learning_rate=0.1, max_depth=-1, min_child_samples=111,\n",
       "               min_child_weight=0.01, min_split_gain=0.0, n_estimators=100,\n",
       "               n_jobs=-1, num_leaves=38, objective=None, random_state=None,\n",
       "               reg_alpha=0, reg_lambda=0.1, silent=True,\n",
       "               subsample=0.3029313662262354, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lgb.LGBMClassifier(colsample_bytree = 0.952164731370897, min_child_samples= 111, \n",
    "#                           min_child_weight= 0.01,num_leaves= 38, reg_alpha= 0, reg_lambda= 0.1, \n",
    "#                           subsample = 0.3029313662262354)\n",
    "lgbm = lgb.LGBMClassifier(colsample_bytree = 0.952164731370897, min_child_samples = 111, \n",
    "        min_child_weight = 0.01, num_leaves = 38, reg_alpha = 0, \n",
    "        reg_lambda = 0.1, subsample = 0.3029313662262354)\n",
    "lgbm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.15, loss='deviance', max_depth=8,\n",
       "                           max_features='log2', max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=0.24545454545454548,\n",
       "                           min_samples_split=0.46363636363636374,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=130,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=0.95, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(subsample= 0.95, n_estimators= 130, \n",
    "                                 min_samples_split= 0.46363636363636374, min_samples_leaf= 0.24545454545454548, \n",
    "                                 max_features= 'log2', max_depth= 8, loss= 'deviance', \n",
    "                                 learning_rate= 0.15, criterion= 'friedman_mse')\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.01,\n",
       "                   n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators= 300, learning_rate= 0.01)\n",
    "abc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingCVClassifier(classifiers=(XGBClassifier(base_score=0.5,\n",
       "                                                booster='gbtree',\n",
       "                                                colsample_bylevel=1,\n",
       "                                                colsample_bynode=1,\n",
       "                                                colsample_bytree=0.4, gamma=0.0,\n",
       "                                                learning_rate=0.1,\n",
       "                                                max_delta_step=0, max_depth=3,\n",
       "                                                min_child_weight=1,\n",
       "                                                missing=None, n_estimators=300,\n",
       "                                                n_jobs=1, nthread=None,\n",
       "                                                objective='binary:logistic',\n",
       "                                                random_state=0, reg_alpha=0,\n",
       "                                                reg_lambda=1,\n",
       "                                                scale_pos_weig...\n",
       "                                                   n_estimators=300, n_jobs=1,\n",
       "                                                   nthread=None,\n",
       "                                                   objective='binary:logistic',\n",
       "                                                   random_state=0, reg_alpha=0,\n",
       "                                                   reg_lambda=1,\n",
       "                                                   scale_pos_weight=1,\n",
       "                                                   seed=None, silent=None,\n",
       "                                                   subsample=1, verbosity=1),\n",
       "                     n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
       "                     shuffle=True, store_train_meta_features=False,\n",
       "                     stratify=True, use_clones=True,\n",
       "                     use_features_in_secondary=True, use_probas=False,\n",
       "                     verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack up all the models above, optimized using xgboost\n",
    "stack_gen = StackingCVClassifier(classifiers=(xgb, abc, gbc, lgbm),\n",
    "                                meta_classifier=xgb,\n",
    "                                use_features_in_secondary=True)\n",
    "stack_gen.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return roc_auc_score(y, y_pred)\n",
    "\n",
    "def blended_predictions(x_test):\n",
    "    return ((0.25 * xgb.predict_proba(x_test)) + \n",
    "            (0.2 * gbc.predict_proba(x_test)) + \n",
    "            (0.05 * abc.predict_proba(x_test)) +\n",
    "            (0.25 * lgbm.predict_proba(x_test)) +\n",
    "            (0.25 * stack_gen.predict_proba(np.array(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971154379541798"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend = blended_predictions(x_test)\n",
    "blend = ((0.25 * xgb.predict(x_test)) + \n",
    "            (0.2 * gbc.predict(x_test)) + \n",
    "            (0.05 * abc.predict(x_test)) +\n",
    "            (0.25 * lgbm.predict(x_test)) +\n",
    "            (0.25 * stack_gen.predict(np.array(x_test))))\n",
    "# pd.DataFrame(blend)\n",
    "blended_score = rmsle(y_test, blend)\n",
    "blended_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y_test, preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.274413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.022292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.011324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.147457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID         1\n",
       "0   1  0.274413\n",
       "1   2  0.022292\n",
       "2   3  0.011324\n",
       "3   4  0.101500\n",
       "4   5  0.147457"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('C:\\\\Users\\\\abhin\\\\Desktop\\\\M571\\\\bankruptcy_Test_X.csv')\n",
    "test1 = test.drop(\"ID\", axis=1)\n",
    "pred_final = pd.DataFrame(blended_predictions(test1))\n",
    "pred_final = pd.concat([test[\"ID\"], pred_final[1]], axis = 1)\n",
    "pd.DataFrame(pred_final).to_csv('C:\\\\Users\\\\abhin\\\\Desktop\\\\M571\\\\d4b_10_2_v4.csv')\n",
    "pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_val = xgb.predict(x_test)\n",
    "# roc_auc_score(y_test, pred_val)\n",
    "# pred_val1 = gbc.predict(x_test1)\n",
    "# roc_auc_score(y_test1, pred_val)\n",
    "# pred_val2 = abc.predict(x_test1)\n",
    "# roc_auc_score(y_test1, pred_val)\n",
    "# pred = xgb.predict(x_test)\n",
    "# roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=pd.read_csv('C:\\\\Users\\\\abhin\\\\Desktop\\\\M571\\\\bankruptcy_Test_X.csv')\n",
    "# test1 = test.drop(\"ID\", axis=1)\n",
    "# pred_final = xgb.predict_proba(test1)\n",
    "# pred_final = pd.DataFrame(pred_final)\n",
    "# pred_final = pd.concat([test[\"ID\"], pred_final[1]], axis = 1)\n",
    "# pred_final.head()\n",
    "# pd.DataFrame(pred_final).to_csv('C:\\\\Users\\\\abhin\\\\Desktop\\\\M571\\\\d4b_10_2_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop outliers\n",
    "# z = np.abs(stats.zscore(temp))\n",
    "# temp = temp[(z < 4).all(axis=1)]\n",
    "# temp.shape\n",
    "\n",
    "# correlated_features = set()\n",
    "# correlation_matrix = temp.corr()\n",
    "# for i in range(len(correlation_matrix .columns)):\n",
    "#     for j in range(i):\n",
    "#         if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "#             colname = correlation_matrix.columns[i]\n",
    "#             correlated_features.add(colname)\n",
    "\n",
    "# temp.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "#separate target and predictors\n",
    "# boruta = [\"Attr2\",\"Attr5\",\"Attr6\",\"Attr9\",\"Attr12\",\"Attr13\",\"Attr16\",\"Attr17\",\"Attr19\",\"Attr23\",\"Attr25\",\n",
    "#           \"Attr26\",\"Attr27\",\"Attr29\",\"Attr34\",\"Attr35\",\"Attr39\",\"Attr46\"]\n",
    "# x_resampled = x_resampled[boruta]\n",
    "# xg_reg = xgb.XGBClassifier(objective ='binary:logistic',alpha = 10, silent=False, \n",
    "#                       scale_pos_weight=1, colsample_bynode=1, colsample_bylevel=1,\n",
    "#                       learning_rate=0.12,  \n",
    "#                       colsample_bytree = 0.8,\n",
    "#                       subsample = 0.8, \n",
    "#                       n_estimators=1000, \n",
    "#                       reg_alpha = 0.3,\n",
    "#                       max_depth=15, \n",
    "#                       gamma=0.5)\n",
    "# xg_reg = XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n",
    "#               learning_rate=0.11, max_delta_step=0.5, max_depth=15,\n",
    "#               min_child_weight=2, missing=None, n_estimators=300, n_jobs=1,\n",
    "#               objective='binary:logistic', random_state=0,\n",
    "#               reg_alpha=0.3, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=False, subsample=0.8, verbosity=1)\n",
    "# xg_reg = XGBClassifier(alpha=0.07, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n",
    "#               learning_rate=0.07, max_delta_step=0,\n",
    "#               max_depth=15, min_child_weight=1.0, missing=None,\n",
    "#               n_estimators=1000, n_jobs=1, nthread=None,\n",
    "#               objective='binary:logistic', random_state=0, reg_alpha=0.3,\n",
    "#               reg_lambda=1, scale_pos_weight=1, seed=None, silent=False,\n",
    "#               subsample=0.8, tree_method='hist', grow_policy='lossguide', max_leaves=50, verbosity=1)\n",
    "# xg_reg = XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.6,\n",
    "#               gamma=1.4, learning_rate=0.4, max_delta_step=0.5,\n",
    "#               max_depth=18, min_child_weight=1.0, missing=None,\n",
    "#               n_estimators=350, n_jobs=4, nthread=None,\n",
    "#               objective='binary:logistic', random_state=0, reg_alpha=0.3,\n",
    "#               reg_lambda=1, scale_pos_weight=1, seed=None, silent=False,\n",
    "#               subsample=0.8, verbosity=1)\n",
    "# xg_reg = XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.7, gamma=2,\n",
    "#               learning_rate=0.1, max_delta_step=1.5, max_depth=17,\n",
    "#               min_child_weight=1.0, missing=None, n_estimators=300, n_jobs=1,\n",
    "#               nthread=None, objective='binary:logistic', random_state=0,\n",
    "#               reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=False, subsample=0.6, verbosity=1)\n",
    "# xg_reg = XGBClassifier(alpha=0, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=1, gamma=0.0, reg_lambda=0,\n",
    "#               learning_rate=0.11, max_delta_step=0.5, max_depth=2,\n",
    "#               min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
    "#               nthread=None, objective='binary:logistic', random_state=0,\n",
    "#               scale_pos_weight=1, seed=None,\n",
    "#               silent=None, subsample=1, verbosity=1)\n",
    "# xg_reg = XGBClassifier(alpha=2.5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=1, gamma=0.5, reg_lambda=4.0,\n",
    "#               learning_rate=0.11, max_delta_step=0.0,\n",
    "#               max_depth=2, min_child_weight=1, missing=None, n_estimators=300,\n",
    "#               n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "#               random_state=0,scale_pos_weight=1,\n",
    "#               seed=None, silent=None, subsample=1, verbosity=1)\n",
    "# params = {\n",
    "#         'min_child_weight': np.arange(0,11,0.5),\n",
    "#         'gamma': np.arange(0,2,0.2),\n",
    "#         'subsample': np.arange(0,1,0.2),\n",
    "#         'colsample_bytree': np.arange(0,1,0.2),\n",
    "#         'max_depth': np.arange(0,27),\n",
    "#         \"learning_rate\": np.arange(0,0.3,0.01)\n",
    "#         }\n",
    "# params = {\n",
    "#         'n_estimators': np.arange(200,600,50),\n",
    "#         'min_child_weight': np.arange(0,7,0.5),\n",
    "#         'gamma': np.arange(0,5,0.2),\n",
    "#         'subsample': np.arange(0,1,0.2),\n",
    "#         'colsample_bytree': np.arange(0,1,0.2),\n",
    "#         'max_depth': np.arange(0,27,2),\n",
    "#         \"learning_rate\": np.arange(0,2,0.2)\n",
    "#         }\n",
    "# boruta = [\"Attr2\",\"Attr5\",\"Attr6\",\"Attr9\",\"Attr12\",\"Attr13\",\"Attr16\",\"Attr17\",\"Attr19\",\"Attr23\",\"Attr25\",\n",
    "#           \"Attr26\",\"Attr27\",\"Attr29\",\"Attr34\",\"Attr35\",\"Attr39\",\"Attr46\"]\n",
    "# imp_features = ['Attr27','Attr5','Attr41','Attr13','Attr37','Attr9','Attr34','Attr6','Attr39','Attr58','Attr26',\n",
    "#                 'Attr46','Attr60','Attr24','Attr9','Attr55','Attr59','Attr25','Attr61','Attr30','Attr1','Attr29']\n",
    "# # load X and y\n",
    "# x, y = df.drop('class',axis=1),df['class']\n",
    "# # define random forest classifier, with utilising all cores and\n",
    "# # sampling in proportion to y labels\n",
    "# forest = RandomForestClassifier(n_jobs=4, class_weight='balanced')\n",
    " \n",
    "# # define Boruta feature selection method\n",
    "# feat_selector = BorutaPy(forest, n_estimators='auto', verbose=2)\n",
    " \n",
    "# # find all relevant features\n",
    "# feat_selector.fit(x.values, y.values)\n",
    "# feat_selector.support_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
